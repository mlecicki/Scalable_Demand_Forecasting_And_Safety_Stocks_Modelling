---
title: "Scalable Demand Forecasting and Safety Stock Modelling using Forecast Error"
author: "Maciej Lecicki"
date: "24.10.2021"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, message = FALSE, warning = FALSE, fig.width = 12, fig.height = 6, fig.pos = 'H')
```

<br/>
<br/>
<br/>

### Purpose and objectives
<br/>

The purpose of this project is to illustrate how to develop robust demand forecasting model based on time series analysis and machine learning in R environment. <br/>

Important goal of this project is to show: <br/>
- potential of open source software like R for demand forecasting,<br/>
- scalability of this approach, namely ease of forecasting automation for multiple products/regions,<br/>
- ability to shape model selection criteria using common and customized metrics,<br/>
- focus on forecast error and confidence intervals as input to scenario planning for supply planning or S&OP process,<br/>
- use of forecast error as 'demand uncertainty' part of Safety Stock calculation.



##### Libraries and data
<br/>
```{r}
library('tidyverse')
library('tidymodels')
library('modeltime')
library('timetk')
library('lubridate')
library('workflowsets')
library('tune')
library('patchwork')
```

Key libraries used in this project are modeltime and tidymodels, both based on 'tidyverse' principles.

```{r}
data <- read_csv('data/elecsupply.csv')
glimpse(data)
range(data$date)
```

Dataset consists of monthly demand for electrical supply for 12 European countries collected between 1990 and 2003.

Let's change table format from wide to long and visualize demand.

```{r}
(elecsupply <- data %>%
  gather(country, elec_demand, Belgium:UK) %>%
  mutate(date = as_date(date, format = '%d.%m.%Y')) %>%
  relocate(country)
)

```

For demand visalization we could use plot_time_series() function (code is included as comments) from timetk package. It provides interactive element and has other advantages too, however I prefer 'good old' gglot2 which I find more elegant alternative if interaction isn't necessary.

```{r}
# elecsupply %>%
#   group_by(country) %>%
#   plot_time_series(date, elec_demand, .facet_ncol = 4, .smooth = FALSE)

elecsupply %>%
  ggplot(aes(x = date, y = elec_demand, group = country, color = country)) +
      geom_line(show.legend = FALSE) +
      theme_minimal() +
  facet_wrap(. ~ country, ncol = 3, scales = 'free_y') +
  geom_smooth(method = 'lm', se = FALSE, show.legend = FALSE)
```

With quick glance at all plots we can tell that demand is seasonal and there's upward trend in demand for electrical supply.


##### Time series modelling
<br/>

The goal we'd like to achieve is development of best fit demand forecast using available resources in R.
The challenge is that we'd like to automate this process and do it at scale, meaning that we'd like to develop best models for all countries.
<br/>

To do that, we'll use modeltime package supported by tidymodels and tidyverse.
Let's start by describing high level the process and list key steps.<br/>

1. Dataset will be split into train and test:<br/>
 - train will be used to build the model,<br/>
 - testset of 24 months will be used to validate acccuracy and select best model,<br/>
 - in addition, test will be used to evaluate expected error that can be cascaded to Safety Stock calculation and scenario planning,<br/>
2. We'll also expand dataset by 12 months to forecast demand for electrical supply; it will be done by refitting best model on full dataset (train + test),<br/>
3. We'll explore 'classic' time series approach and machine learning based model to develop demand forecasting model which minimizes selected forecast error metric,<br/>
4. We'll look into standard error metrics available in R libraries and enrich metrics selection through development of own, custom metrics.<br/>

Findings at each step of the process will be visualized using ggplot2. This can also be used as input to Shiny web app (if required to be shared with user that doesn't have access to R environment).


##### Train, test and future datasets
<br/>

```{r}
nested_data_tbl <- elecsupply %>%
  group_by(country) %>%
  extend_timeseries(
    .id_var = country,
    .date_var = date,
    .length_future = 12 # this will be our forecasting horizon (in months)
  ) %>%
  nest_timeseries(
    .id_var = country,
    .length_future = 12
  ) %>%
  split_nested_timeseries(
    .length_test = 24 # test dataset (periods are months)
  )

nested_data_tbl
```

Actual and future data is nested inside newly created object and grouped by country. In addition we have information about split for train and test sets.<br/>

To get inside nested data we can use unnest() function from tidyr library. Example is shown below.
I encourage to also run commented out code to see the difference in output.

```{r}
nested_data_tbl %>%
  filter(country == 'Denmark') %>%
  unnest(.actual_data) %>%
  select(country:elec_demand)

# nested_data_tbl %>%
#   filter(country == 'Denmark') %>%
#   unnest(.actual_data)
```

Nesting data in a form of tibble is very convenient and helpful from the point of view of model scalability. This concept will be used heavily in this project.


##### Time series modelling
<br/>


First, we'll build demand forecasting model using Machine Learning. Functions from modeltime library will be used in conjunction with tidymodels package.<br/>



Machine Learning based models don't recognize date based information, hence we need to recode date into 'ML friendly' calendar based qualitative features. Before that, to follow tidymodels standard we'll prepare recipe used in the model.<br/>
There's also a few other important steps taken in below code related to data pre-processing. All steps have relevant comments.

```{r}
rec_xgb <- recipe(elec_demand ~ ., extract_nested_train_split(nested_data_tbl)) %>%
  step_timeseries_signature(date) %>% # create dummy calendar features based on date
  step_rm(date) %>% # remove date column - not require anymore
  step_zv(all_predictors()) %>% # remove zero value predictors (if there are any)
  step_dummy(all_nominal(), one_hot = TRUE) # dummy predictors that are character data
```


We can check our recipe using bake() formula.

```{r}
bake(prep(rec_xgb), extract_nested_train_split(nested_data_tbl))
```

We can see that step_series_signature() formula created many calendar features and ALL of them have been passed to recipe.<br/>
At this point we could at a question if we need all of them and if all of them make sense taking into account granularity (frequency) of original date and its format (1st day of the month, month and year) or would they rather confuse our machine learning model?<br/>
I think it's the latter and therefore let's narrow features to correct ones taking into above.<br/>

Let's re-write our recipe and remove unwanted columns in step_rm() function, where we can list what needs to be de-selected, or by using ! and - operators, what needs to be kept. Please notice that since character features are removed, one hot encoding is not needed anymore.

```{r}
rec_xgb <- recipe(elec_demand ~ ., extract_nested_train_split(nested_data_tbl)) %>%
  step_timeseries_signature(date) %>%
  step_rm(!elec_demand, -c(date_year.iso:date_month)) %>%
  step_zv(all_predictors()) # remove zero value predictors (if there are any)
  # step_dummy(all_nominal(), one_hot = TRUE) # not required anymore

```

Let's take a look corrected recipe.

```{r}
bake(prep(rec_xgb), extract_nested_train_split(nested_data_tbl))
```

By default, it returns train set for first aggregation field (country in our case). It's possible however to check recipe for any country as shown below.

```{r}
bake(prep(rec_xgb), extract_nested_train_split(nested_data_tbl %>%
                                                 filter(country == 'Denmark')))
```

We now have recipe for Machine Learning. Classic time series algorithms don't require data preprocessing thus saving recipe to an object.
<br/>
Next step is create workflows, which are containers that aggregate information required to fit and predict from a model.
We'll build one machine learning workflow and four others that cover more standard time series based models.

1. boosted_tree workflow.

Like any other ML algorithm, xgboost has hyperparameters. Hyperparameters tuning is out of scope of this project, however we'll show how it's done using tidymodels library. 
Detailed process can be found under below link:<br/>
https://towardsdatascience.com/dials-tune-and-parsnip-tidymodels-way-to-create-and-tune-model-parameters-c97ba31d6173

Tuning is done using our recipe which is based on a subset of our data (first country).

```{r}
tune_spec <- boost_tree(
  learn_rate = tune(),
  trees = tune(),
  mtry = tune(),
  tree_depth = tune()
) %>%
  set_engine('xgboost') %>%
  set_mode('regression')
#
folds_3 <- vfold_cv(extract_nested_train_split(nested_data_tbl),
                    v = 3)

wflw_xgb_1 <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(rec_xgb)

set.seed(300)
wflw_xgb_1_tune <- wflw_xgb_1 %>%
  tune_grid(
    resamples = folds_3,
    grid = 3
  )

wflw_xgb_1_tuned <- finalize_workflow(wflw_xgb_1,
                                      select_best(wflw_xgb_1_tune))

# optimal hyperparameters
wflw_xgb_1_tuned
```

Let's use some of these values in the workflow.

```{r}
wflw_xgb <- workflow() %>%
  add_model(boost_tree("regression",
                       mtry = 3,
                       trees = 1085,
                       tree_depth = 10,
                       learn_rate = 0.03) %>%
              set_engine("xgboost")) %>%
  add_recipe(rec_xgb)
```

2. Temporal Hierarchical Forecasting (THIEF) workflow<br/>

All key information about it can be found under below link:<br/>
https://robjhyndman.com/hyndsight/thief/

As for the worklfow itself, notice difference in recipe between xgboost and non-ML workflows.

```{r}
wflw_thief <- workflow() %>%
  add_model(temporal_hierarchy() %>%
              set_engine("thief")) %>%
  add_recipe(recipe(elec_demand ~ ., extract_nested_train_split(nested_data_tbl)))
```

In above code, 'elec_demand ~ .' formula could be replaced by 'elec_demand ~ date' as date is the only predictor (historic demand). The same applies to all remaining recipies in below workflows.

3. Exponential smoothing workflow

```{r}
wfl_exp_s <- workflow() %>%
  add_model(exp_smoothing() %>%
              set_engine("ets")) %>%
  add_recipe(recipe(elec_demand ~ date, extract_nested_train_split(nested_data_tbl)))
```


4. (S)ARIMA workflow

```{r}
wfl_arima <- workflow() %>%
  add_model(arima_reg() %>%
              set_engine("auto_arima")) %>%
  add_recipe(recipe(elec_demand ~ ., extract_nested_train_split(nested_data_tbl)))
```

auto_arima engine is used which means that autoregression, moving average and seasonal parameters will be auto-tuned.

5. Prophet workflow

"Prophet implements a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well."

source: https://cran.r-project.org/web/packages/prophet/index.html


```{r}
wfl_prophet <- workflow() %>%
  add_model(prophet_reg() %>%
              set_engine("prophet")) %>%
  add_recipe(recipe(elec_demand ~ ., extract_nested_train_split(nested_data_tbl)))
```


Now that we have all workflows, they can be fit by using modeltime_nested_fit() function from modeltime package. The beauty of this solution is its scalability. <br/>
What it means? It means that we'll in fact build five different models per country.
<br/>
Before that, good practice is to fit them on a single dataset (country) and inspect for any potential errors (usually driven by data quality, namely completeness). Let's do it!<br/>

```{r echo = FALSE}
try_sample_tbl <- nested_data_tbl %>%
  dplyr::slice(2) %>% #2nd (nested) country
  modeltime_nested_fit(
    model_list = list( # list of workflows
      wflw_xgb,
      wflw_thief,
      wfl_exp_s,
      wfl_arima,
      wfl_prophet
    ),
    control = control_nested_fit(
      verbose = TRUE, # setting this to TRUE allows detailed progress check, 
                      # whereas value FALSE gives high level progress
      allow_par = FALSE
    )
  )
```

To check for errors we can use below code.

```{r}
try_sample_tbl %>%
  extract_nested_error_report()
```

Our workflows are errors-free so let's fit them on full dataset. Fitting models can take some time. We can shorten it allowing parallel run on PC's cores. To do that (and first to check for number of cores, functions from parallel library can be used - commented out in below code).
Alternatively we can set allow_par argument to TRUE will have similar effect (although recommended is using parallel_start() function with relevant number of cores.

```{r}
parallel::detectCores()
parallel_start(4)

nested_modeltime_tbl <- nested_data_tbl %>%
  modeltime_nested_fit(
    model_list = list(
      wflw_xgb,
      wflw_thief,
      wfl_exp_s,
      wfl_arima,
      wfl_prophet
    ),
    control = control_nested_fit(
      verbose = FALSE,
      allow_par = FALSE
    )
  )
parallel_stop()
```

We haven't specified any metrics in modeltime_nested_fit() function hence default set of numeric metrics for regression models will be provided. Customized list of metrics can be specified through metric_set parameter (we'll get back to this at later stage).<br/>


##### Inspection of models accuracy

Having fit models for all countries we can asses their accuracy using available metrics. Evaluation is done on TEST set.
table_nested_test_accuracy() function creates nice html table with all metrics per model per country (I'll comment it out to simplify output and the size of .md file)

```{r}
nested_modeltime_tbl %>%
  extract_nested_test_accuracy() # %>%
  # table_modeltime_accuracy()
```

Forecast generated by all models can be visualized on test set using below code. Plot generated by plot_modeltime_forecast function can be interactive when '.interactive' argument is set to TRUE. It's a nice feature as it allows zooming in.

```{r}
nested_modeltime_tbl %>%
  extract_nested_test_forecast() %>%
  group_by(country) %>%
  plot_modeltime_forecast(.facet_ncol = 4, # when filter for country is removed
                          .conf_interval_show = FALSE,
                          .legend_show = TRUE,
                          .interactive = FALSE,
                          .title = 'Forecasts for test set')
```
Plots are busy as all models are displayed, however at at this stage we can also select best model per country. <br/> Default error ruducing metric is 'rmse' but this can be also specified through 'metric' parameter inside modeltime_nested_select_best() function.

```{r}
nested_best_tbl <- nested_modeltime_tbl %>%
  modeltime_nested_select_best() # try metric = "mae"
```

Forecast generated by best model per country can also be plotted.

```{r}
nested_best_tbl %>%
  extract_nested_test_forecast() %>%
  # filter(country = 'Denmark') %>%
  group_by(country) %>%
  plot_modeltime_forecast(.facet_ncol = 4,
                          .conf_interval_show = TRUE,
                          .legend_show = TRUE,
                          .interactive = FALSE,
                          .title = 'Forecast from best model per country.'
                          )
```

Visual inspection of forecast for test set is important step in demand forecasting. Equally important is inspection of forecast errors which is critical input to Supply Planning for scenario planning and safety stock calculation.<br/>
In addition to that, we can also learn more about our models and also reveal some weaknesses of a single error metric (however good it is, like rmse) which can lead to further improvement or change of demand forecasting method.<br/>
As we'll see shortly, it may require building customized metrics (luckily, yardstick library provides all necessary tools to do that).

We'll use ggplot to explore forecast errors. Object nested_best_tbl holds information about best models. 

```{r}
nested_best_tbl_extract <- nested_best_tbl %>%
    extract_nested_test_forecast()
```

With a little bit of data transformation we can get all we need to plot errors and evaluate best models from that perspective. <br/>

```{r}
actual <- nested_best_tbl_extract %>%
  filter(.key == 'actual')

pred <- nested_best_tbl_extract %>%
  filter(.key == 'prediction')

vis_table <- actual %>%
  left_join(pred, 
            by = c('country', '.index'),
            keep = FALSE) %>%
  filter(.key.y == 'prediction') %>%
  rename(conf_low = .conf_lo.y,
         conf_high = .conf_hi.y,
         actual = .value.x,
         prediction = .value.y) %>%
  group_by(country) %>%
  mutate(error = actual - prediction,
         BIAS_cumsum = cumsum(actual - prediction)) %>%
  select(!c(.model_id.x, .model_desc.x, .conf_lo.x, .conf_hi.x)
         )
```

Let's again look into best models, but this time we'll only inspect test set horizon which should greatly improve readability of our plot.

```{r}
vis_table %>%
  filter(.key.y == 'prediction') %>%
  ggplot(group = country) +
  geom_line(aes(x = .index, y = actual)) +
  geom_line(aes(x = .index, y = prediction, color = .model_desc.y)) +
  geom_line(aes(x = .index, y = conf_low), color = 'lightgrey') +
  geom_line(aes(x = .index, y = conf_high), color = 'lightgrey') +
  theme_minimal() +
  facet_wrap(.~ country, ncol = 3, scales = 'free_y') +
  labs(color = 'model') +
  theme(legend.position = 'bottom', plot.title = element_text(hjust = 0.5)) +
  ggtitle('Best models fit over test set horizon.')
```

It makes a difference, doesn't it? Coloured line represent best model based on rmse metric, lightgrey lines mark lower and upper boundaries of confidence interval (95%).
<br/>

**There are 2 interesting observations from above plots.**<br/>
**Firstly, best results (minimizing RMSE) are achieved through different models, depending on a country. This shows that 'one-fit all' approach doesn't work in demand forecasting.**<br/>
**Secondly, in some countries, Machine Learning based models deliver best results, confirming its value for demand forecasting.**

Finally, we can inspect forecast errors. First, let's visualize their distribution.

```{r}
vis_table %>%
  group_by(country) %>%
  ggplot(aes(error, fill = .model_desc.y)) +
  geom_histogram() +
  theme_minimal() +
  facet_wrap(.~ country, 
             ncol = 3, 
             scales = 'free'
             ) +
  labs(fill = 'model') +
  theme(legend.position = 'bottom', 
        plot.title = element_text(hjust = 0.5),
        axis.title.x = element_blank()) +
  ggtitle('Best models: distribution of errors measured on test set horizon.')
```

Indeed, distribution of errors in some countries is clearly skewed either to left or right. Extreme example of this is Netherlands and Denmark, which suggests presence of positive BIAS.<br/>
This can be easier to notice using geom_density instead of geom_histogram since we only have 24 discrete observations. As a reminder, area under density curve is 1 (which is 100% of all probabilities for value on x-axis).

```{r}
vis_table %>%
  group_by(country) %>%
  ggplot(aes(error, fill = .model_desc.y)) +
  geom_density() +
  theme_minimal() +
  facet_wrap(.~ country, 
             ncol = 3, 
             scales = 'free'
             ) +
  labs(fill = 'model') +
  theme(legend.position = 'bottom',
        plot.title = element_text(hjust = 0.5),
        axis.title.x = element_blank()) +
  ggtitle('Best models: density of errors measured on test set horizon.')
```

Let's confirm our findings plotting BIAS (cumulative sum of error calculated as actual - forecast) to check for any patterns of CONSISTENT deviation from actual demand from test set.

```{r}
vis_table %>%
  ggplot(group = country) +
  geom_point(aes(x = .index, y = BIAS_cumsum, color = .model_desc.y)) +
  geom_line(aes(x = .index, y = BIAS_cumsum, color = .model_desc.y)) +
  theme_minimal() +
  facet_wrap(.~ country, ncol = 3, scales = 'free_y') +
  labs(color = 'model') +
  theme(legend.position = 'bottom', 
        plot.title = element_text(hjust = 0.5),
        axis.title.x = element_blank()) +
  ggtitle('Best models: cumulative monthly errors over test set horizon.')
```

We can indeed see clear BIAS in Netherlands and Denmark. It also appears that most models are biased, however overall direction is often driven by high error in one of months.<br/>
For comparison, let's look at errors and if they oscillate around 0.

```{r}
vis_table %>%
  ggplot(group = country) +
  geom_point(aes(x = .index, y = error, color = .model_desc.y)) +
  geom_line(aes(x = .index, y = error, color = .model_desc.y)) +
  geom_line(aes(x = .index, y = 0), color = 'black') +
  theme_minimal() +
  facet_wrap(.~ country, ncol = 3, scales = 'free_y') +
  labs(color = 'model') +
  theme(legend.position = 'bottom', 
        plot.title = element_text(hjust = 0.5),
        axis.title.x = element_blank()) +
  ggtitle('Best models: monthly errors over test set horizon.')
```

Above plot confirms positive BIAS in Netherlands and Denmark and negative BIAS in UK over majority of test horizon. Errors in other market oscillate around 0, which indicates that there's no major problem with BIAS.<br/>

Selection of model reducing BIAS is not possible in modeltime library simply because we're restricted by available metrics.<br/>
We can however create custom metrics with a help of yardstick library which provides a standard template for this. To present this functionality, we'll build a few simple BIAS-oriented metrics and some other demand forecasting accuracy-oriented metrics.


We'll create 4 metrics following same steps:<br/>

1. tracking signal - arithmetic sum of errors divided by mean absolute deviation (error); this is approach described in APICS CPIM.

```{r}

# first step is to create vector version of function

track_sig_v <- function(truth, estimate, na_rm = TRUE, ...) {
  
  t_sig_impl <- function(truth, estimate) {
    sum(truth - estimate)/mean(abs(truth - estimate))
  }
  metric_vec_template(
    metric_impl = t_sig_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )
}

# next step is data frame implementation of vector version function  


track_sig <- function(data, ...) {
  UseMethod("track_sig")
}

track_sig <- new_numeric_metric(track_sig, direction = "zero")

track_sig.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  metric_summarizer(
    metric_nm = "track_sig",
    metric_fn = track_sig_v,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate),
    na_rm = na_rm,
    ...
  )
}
```

2. BIAS numeric - if Actual - Prediction < 0 then -1 (negative BIAS), if = 0 then 0 (no BIAS) else 1 (positive BIAS)

```{r}
bias_n_v <- function(truth, estimate, na_rm = TRUE, ...) {
  
  bias_n_v_impl <- function(truth, estimate) {
    
    m <- truth - estimate
    m[m < 0] <- -1
    m[m > 0] <- 1
    return(sum(m))
  }
  metric_vec_template(
    metric_impl = bias_n_v_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )
}

bias_n <- function(data, ...) {
  UseMethod("bias_n")
}

bias_n <- new_numeric_metric(bias_n, direction = "zero")

bias_n.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  metric_summarizer(
    metric_nm = "bias_n",
    metric_fn = bias_n_v,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate),
    na_rm = na_rm,
    ...
  )
}
```

3. BIAS (cumulative sum of errors)

```{r}
bias_cumsum_v <- function(truth, estimate, na_rm = TRUE, ...) {
  
  bias_cumsum_v_impl <- function(truth, estimate) {
    
    m <- truth - estimate
    n <- cumsum(m)
    return(n[length(n)])
  }
  metric_vec_template(
    metric_impl = bias_cumsum_v_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )
}

bias_cumsum <- function(data, ...) {
  UseMethod("bias_cumsum")
}

bias_cumsum <- new_numeric_metric(bias_cumsum, direction = "zero")

bias_cumsum.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  metric_summarizer(
    metric_nm = "bias_cumsum",
    metric_fn = bias_cumsum_v,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate),
    na_rm = na_rm,
    ...
  )
}
```

4. Accuracy - percentage of errors falling within +- 5% threshold<br/>


```{r}
acc_95_v <- function(truth, estimate, na_rm = TRUE, ...) {

  acc_95_v_impl <- function(truth, estimate) {

    m <- truth/estimate
    m[m < 0.95 | m > 1.05] <- 0
    m[m >= 0.95 & m <= 1.05] <- 1
    return(sum(m)/length(m))
  }
  metric_vec_template(
    metric_impl = acc_95_v_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )
}

acc_95 <- function(data, ...) {
  UseMethod("acc_95")
}

acc_95 <- new_numeric_metric(acc_95, direction = "maximize")

acc_95.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  metric_summarizer(
    metric_nm = "acc_95",
    metric_fn = acc_95_v,
    data = data,
    truth = !! enquo(truth),
    estimate = !! enquo(estimate),
    na_rm = na_rm,
    ...
  )
}
```

Such created custom metrics can be passed to metric_set argument for best model selection criteria.

TO BE CONTINUED.

